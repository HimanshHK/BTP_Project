{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh7728l0GFyD",
        "outputId": "16247859-a3bb-44a0-f8c5-0720d16da4f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: scikit-learn-extra in c:\\users\\himanshu\\appdata\\roaming\\python\\python311\\site-packages (0.3.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\himanshu\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn-extra) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\himanshu\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn-extra) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.23.0 in c:\\users\\himanshu\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn-extra) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\himanshu\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\himanshu\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn-extra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WWLpnThzKpfx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rn8Qm_Y_Ku0G"
      },
      "outputs": [],
      "source": [
        "# Load the malware dataset\n",
        "data = pd.read_csv('Malware_dataset.csv')\n",
        "# Randomly select 100 rows from the dataset\n",
        "data = data.sample(n=10000, random_state=42)\n",
        "data.drop('hash', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBr678AOK-mI",
        "outputId": "698d015b-eaae-433f-de2d-7f623b1b5718"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "millisecond          0\n",
            "classification       0\n",
            "state                0\n",
            "usage_counter        0\n",
            "prio                 0\n",
            "static_prio          0\n",
            "normal_prio          0\n",
            "policy               0\n",
            "vm_pgoff             0\n",
            "vm_truncate_count    0\n",
            "task_size            0\n",
            "cached_hole_size     0\n",
            "free_area_cache      0\n",
            "mm_users             0\n",
            "map_count            0\n",
            "hiwater_rss          0\n",
            "total_vm             0\n",
            "shared_vm            0\n",
            "exec_vm              0\n",
            "reserved_vm          0\n",
            "nr_ptes              0\n",
            "end_data             0\n",
            "last_interval        0\n",
            "nvcsw                0\n",
            "nivcsw               0\n",
            "min_flt              0\n",
            "maj_flt              0\n",
            "fs_excl_counter      0\n",
            "lock                 0\n",
            "utime                0\n",
            "stime                0\n",
            "gtime                0\n",
            "cgtime               0\n",
            "signal_nvcsw         0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Define the number of clients\n",
        "num_clients = 3\n",
        "\n",
        "# print(data)\n",
        "# check for missing values\n",
        "print(data.isnull().sum())\n",
        "\n",
        "\n",
        "\n",
        "# fill missing values with mean of the column\n",
        "# data.fillna(data.mean(), inplace=True)\n",
        "\n",
        "# get the number of unique values in each column\n",
        "unique_counts = data.nunique()\n",
        "\n",
        "# get the list of columns to drop\n",
        "to_drop = [col for col in unique_counts.index if unique_counts[col] == len(data)]\n",
        "\n",
        "# drop the columns\n",
        "data.drop(to_drop, axis=1, inplace=True)\n",
        "\n",
        "# Split the dataset into features and labels\n",
        "X = data.drop('classification', axis=1)\n",
        "y = data['classification']\n",
        "# print(y)\n",
        "\n",
        "# converting data to comparable data\n",
        "# one_hot_encoder = OneHotEncoder()\n",
        "# one_hot_encoded = one_hot_encoder.fit_transform(X.iloc[:, 0].values.reshape(-1, 1)).toarray()\n",
        "# print(one_hot_encoded.shape)\n",
        "# X = pd.concat([X.drop('hash', axis=1), pd.DataFrame(one_hot_encoded)], axis=1)\n",
        "# X.columns = X.columns.astype(str)\n",
        "# X = X.values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N53DTfKgLQ_F"
      },
      "outputs": [],
      "source": [
        "# Partition the dataset among the clients\n",
        "X_partitions = []\n",
        "y_partitions = []\n",
        "\n",
        "for i in range(num_clients):\n",
        "    # Split the dataset randomly\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=i)\n",
        "    \n",
        "    # Append the training set to the partitions\n",
        "    X_partitions.append(X_train)\n",
        "    y_partitions.append(y_train)\n",
        "    \n",
        "# Convert the partitions to numpy arrays\n",
        "X_partitions = [pd.DataFrame(X) for X in X_partitions]\n",
        "y_partitions = [pd.DataFrame(y) for y in y_partitions]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch in c:\\users\\himanshu\\appdata\\roaming\\python\\python311\\site-packages (2.0.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\himanshu\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.12.3)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\himanshu\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\himanshu\\appdata\\roaming\\python\\python311\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\himanshu\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\himanshu\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\himanshu\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\himanshu\\appdata\\roaming\\python\\python311\\site-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Attention.__init__() got an unexpected keyword argument 'input_size'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\HIMANSHU\\Documents\\BTP_Project\\Attention Value.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/Documents/BTP_Project/Attention%20Value.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m local_model \u001b[39m=\u001b[39m kmedoids\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/Documents/BTP_Project/Attention%20Value.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Add attention mechanism to the local model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/Documents/BTP_Project/Attention%20Value.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m attention_model \u001b[39m=\u001b[39m Attention(input_size\u001b[39m=\u001b[39;49mX_partitions[i]\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/Documents/BTP_Project/Attention%20Value.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m local_model\u001b[39m.\u001b[39mattention_model \u001b[39m=\u001b[39m attention_model\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/Documents/BTP_Project/Attention%20Value.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Train the local model on the assigned data points\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:445\u001b[0m, in \u001b[0;36mModule.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[39m# Backward compatibility: no args used to be allowed when call_super_init=False\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_super_init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mbool\u001b[39m(kwargs):\n\u001b[1;32m--> 445\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.__init__() got an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(kwargs))))\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_super_init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mbool\u001b[39m(args):\n\u001b[0;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.__init__() takes 1 positional argument but \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m were\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    450\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39m given\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39mlen\u001b[39m(args) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n",
            "\u001b[1;31mTypeError\u001b[0m: Attention.__init__() got an unexpected keyword argument 'input_size'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def _init_(self, input_size):\n",
        "        super(Attention, self)._init_()\n",
        "        self.attention_weights = nn.Linear(input_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attention_scores = self.attention_weights(x)\n",
        "        attention_scores = torch.softmax(attention_scores, dim=0)\n",
        "        attended_features = torch.sum(x * attention_scores, dim=0)\n",
        "        return attended_features\n",
        "# Train the local model on the current partition\n",
        "kmedoids = KMedoids(n_clusters=1, random_state=i)\n",
        "kmedoids.fit(X_partitions[i])\n",
        "local_model = kmedoids\n",
        "\n",
        "# Add attention mechanism to the local model\n",
        "attention_model = Attention(input_size=X_partitions[i].shape[1])\n",
        "local_model.attention_model = attention_model\n",
        "# Train the local model on the assigned data points\n",
        "for j in range(local_models[i].n_clusters):\n",
        "    # Select the data points assigned to the current cluster\n",
        "    cluster_data = X_partitions[i][assignments == j]\n",
        "\n",
        "    # Apply attention mechanism\n",
        "    attended_data = local_model.attention_model(cluster_data)\n",
        "\n",
        "    # Train the local model on the attended data points\n",
        "    local_model = KMedoids(n_clusters=1, random_state=i)\n",
        "    local_model.fit(attended_data)\n",
        "    local_models[i] = local_model\n",
        "# Apply attention mechanism to the global model\n",
        "global_model.attention_model = attention_model\n",
        "\n",
        "# Use the global model for prediction\n",
        "attended_X_test = global_model.attention_model(X_test)\n",
        "y_pred = np.argmin(pairwise_distances(attended_X_test, global_model.reshape(1, -1)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcaEEMtIN24G"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import pairwise_distances\n",
        "# from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# Initialize the global model\n",
        "global_model = None\n",
        "\n",
        "# Define the number of clusters (i.e., the number of local models)\n",
        "num_clusters = len(X_partitions)\n",
        "\n",
        "# Train local models\n",
        "local_models = []\n",
        "for i in range(num_clusters):\n",
        "    # Train the local model on the current partition\n",
        "    kmedoids = KMedoids(n_clusters=1, random_state=i)\n",
        "    kmedoids.fit(X_partitions[i])\n",
        "    local_model = kmedoids\n",
        "\n",
        "    # Add the local model to the list of local models\n",
        "    local_models.append(local_model)\n",
        "\n",
        "# Perform federated learning\n",
        "for epoch in range(num_epochs):\n",
        "    # Aggregate the local models\n",
        "    cluster_centers = [model.cluster_centers_ for model in local_models]\n",
        "    global_model = np.mean(cluster_centers, axis=0)\n",
        "\n",
        "    # Train the local models on the global model\n",
        "    for i in range(num_clusters):\n",
        "        # Compute the distance between the global model and the local data\n",
        "        # Reshape input data to a 2D array with one column\n",
        "        X_2d = X_partitions[i].values.reshape(-1, 33)\n",
        "\n",
        "        # Compute distances to cluster centers\n",
        "        distances = pairwise_distances(X_2d, global_model.reshape(1, -1))\n",
        "\n",
        "        # Assign each data point to the nearest cluster\n",
        "        assignments = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Train the local model on the assigned data points\n",
        "        for j in range(local_models[i].n_clusters):\n",
        "            # Select the data points assigned to the current cluster\n",
        "            cluster_data = X_partitions[i][assignments == j]\n",
        "\n",
        "            # Train the local model on the selected data points\n",
        "            local_model = KMedoids(n_clusters=1, random_state=i)\n",
        "            local_model.fit(cluster_data)\n",
        "            local_models[i] = local_model\n",
        "\n",
        "# Use the global model for prediction\n",
        "y_pred = np.argmin(pairwise_distances(X_test, global_model.reshape(1, -1)), axis=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "h0ve_6oKtDta",
        "outputId": "14429310-41f0-419b-b19c-53b6b55e005a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'y_pred' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\HIMANSHU\\Documents\\BTP_Project\\Attention Value.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/Documents/BTP_Project/Attention%20Value.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/Documents/BTP_Project/Attention%20Value.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Compute confusion matrix\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/Documents/BTP_Project/Attention%20Value.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m cm \u001b[39m=\u001b[39m confusion_matrix(y_test_all, y_pred)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/Documents/BTP_Project/Attention%20Value.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Compute precision\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/Documents/BTP_Project/Attention%20Value.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m precision \u001b[39m=\u001b[39m precision_score(y_test_all, y_pred)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
          ]
        }
      ],
      "source": [
        "label_map = {'benign': 0, 'malware': 1}\n",
        "y_test_all = np.array([label_map[label] for label in y_test])\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test_all, y_pred)\n",
        "\n",
        "# Compute precision\n",
        "precision = precision_score(y_test_all, y_pred)\n",
        "\n",
        "# Compute recall\n",
        "recall = recall_score(y_test_all, y_pred)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test_all, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "# Plot the confusion matrix with values inside it\n",
        "sns.set(font_scale=1.4)\n",
        "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, cmap='Blues', fmt='g')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSZsUipT0FIQ",
        "outputId": "bd087b6c-5c3b-43d8-f35b-760d8474efc0"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'num_clusters' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\HIMANSHU\\Documents\\BTP_Project\\Attention Value.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/Documents/BTP_Project/Attention%20Value.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Define the local models\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/Documents/BTP_Project/Attention%20Value.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m local_models \u001b[39m=\u001b[39m []\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/Documents/BTP_Project/Attention%20Value.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_clusters):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/Documents/BTP_Project/Attention%20Value.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# Train the local model on the current partition using DBSCAN\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/Documents/BTP_Project/Attention%20Value.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     dbscan \u001b[39m=\u001b[39m DBSCAN(eps\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, min_samples\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/Documents/BTP_Project/Attention%20Value.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     dbscan\u001b[39m.\u001b[39mfit(X_partitions[i])\n",
            "\u001b[1;31mNameError\u001b[0m: name 'num_clusters' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Define the local models\n",
        "local_models = []\n",
        "for i in range(num_clusters):\n",
        "    # Train the local model on the current partition using DBSCAN\n",
        "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "    dbscan.fit(X_partitions[i])\n",
        "    local_model = dbscan\n",
        "\n",
        "    # Add the local model to the list of local models\n",
        "    local_models.append(local_model)\n",
        "\n",
        "# Define the global model\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.dense1 = nn.Linear(num_features, 64)\n",
        "        self.dense2 = nn.Linear(64, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.dense1(x))\n",
        "        x = self.dense2(x)\n",
        "        return x\n",
        "\n",
        "global_model = MyModel(num_features=X.shape[1])\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = optim.SGD(global_model.parameters(), lr=0.1)\n",
        "loss_fn = F.cross_entropy\n",
        "\n",
        "# Train the global model using federated learning\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # Update the global model using local models\n",
        "    for i in range(len(X_partitions)):\n",
        "        # Get the data from the current partition\n",
        "        x = torch.from_numpy(X_partitions[i].values).float()\n",
        "\n",
        "        label_encoder = LabelEncoder()\n",
        "        y = label_encoder.fit_transform(y_partitions[i])\n",
        "        y = torch.from_numpy(y).long()\n",
        "\n",
        "        # Compute the local gradients and update the global model\n",
        "        local_model = local_models[i]\n",
        "        preds = local_model.fit_predict(x)\n",
        "        mask = preds != -1\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(global_model(x[mask]), y[mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate the global model\n",
        "    with torch.no_grad():\n",
        "      x = torch.from_numpy(X.to_numpy()).float()\n",
        "      y = torch.from_numpy(y.numpy()).long()\n",
        "      num_samples = min(x.shape[0], y.shape[0], 8000)\n",
        "      indices = torch.randperm(num_samples)\n",
        "      x = x[indices]\n",
        "      y = y[indices]\n",
        "\n",
        "      # Compute the loss and accuracy\n",
        "      loss = loss_fn(global_model(x), y)\n",
        "      preds = torch.argmax(global_model(x), dim=1)\n",
        "      accuracy = torch.mean((preds == y).float())\n",
        "      precision = torch.sum((preds == y).float() * (y == 1).float()) / torch.sum((preds == 1).float())\n",
        "\n",
        "      # Print the epoch, loss, accuracy, and precision\n",
        "      print(f'Epoch {epoch + 1}, Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}, Precision: {precision.item():.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'numpy.ndarray' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\HIMANSHU\\OneDrive\\Documents\\BTP\\Attention Value.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/OneDrive/Documents/BTP/Attention%20Value.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     y \u001b[39m=\u001b[39m y[indices]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/OneDrive/Documents/BTP/Attention%20Value.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# Compute the predictions\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/OneDrive/Documents/BTP/Attention%20Value.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(global_model(x), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/OneDrive/Documents/BTP/Attention%20Value.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Create the confusion matrix\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HIMANSHU/OneDrive/Documents/BTP/Attention%20Value.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m conf_matrix \u001b[39m=\u001b[39m confusion_matrix(y, preds)\n",
            "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# After evaluating the global model, compute predictions and true labels\n",
        "with torch.no_grad():\n",
        "    x = torch.from_numpy(X.to_numpy()).float()\n",
        "    y = torch.from_numpy(y.numpy()).long()\n",
        "    num_samples = min(x.shape[0], y.shape[0], 8000)\n",
        "    indices = torch.randperm(num_samples)\n",
        "    x = x[indices]\n",
        "    y = y[indices]\n",
        "\n",
        "    # Compute the predictions\n",
        "    preds = torch.argmax(global_model(x), dim=1)\n",
        "\n",
        "# Create the confusion matrix\n",
        "conf_matrix = confusion_matrix(y, preds)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
